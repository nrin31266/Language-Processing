# src/kafka/consumer.py

import asyncio
import json
from src.kafka.config import create_kafka_consumer
from src.kafka.event import (
    LessonGenerationRequestedEvent,
    LessonProcessingStepUpdatedEvent
)
from src.kafka.producer import (
    publish_lesson_processing_step_updated
)
from src.enum import LessonProcessingStep, LessonSourceType
from confluent_kafka import KafkaError
from src.kafka.topic import LESSON_GENERATION_REQUESTED_TOPIC, LESSON_PROCESSING_STEP_UPDATED_TOPIC
import uuid
from src import dto
from src.services import media_service
from src.s3_storage import cloud_service
from src.services import ai_job_service
from src.services.lesson_service import lessonParseAiMetaData
from src.services.file_service import fetch_json_from_url
async def handleLessonGenerationRequested(event: LessonGenerationRequestedEvent):
    """X·ª≠ l√Ω khi c√≥ y√™u c·∫ßu t·∫°o b√†i h·ªçc."""
    print(f"üì• Nh·∫≠n LessonGenerationRequestedEvent: {event}")
    try:
        # Cho 3s cho h·ªá th·ªëng ·ªïn ƒë·ªãnh
        await asyncio.sleep(3)
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        isSkip = False
        metadata : dto.AiMetadataDto = None
        try:
            fileMetadata = await fetch_json_from_url(event.ai_meta_data_url)
            metadata = lessonParseAiMetaData(fileMetadata)
            print(f"‚úÖ Fetched AI meta data from URL {event.ai_meta_data_url}: {metadata}")
        except Exception as e:
            print(f"‚ùå Failed to fetch AI meta data from URL {event.ai_meta_data_url}: {e}")
            metadata = dto.AiMetadataDto()   # T·∫°o object r·ªóng ƒë·ªÉ tr√°nh None

        
        
        
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        
        if metadata.source_fetched is None or event.is_restart == True:
            # STEP 1: Download audio t·ª´ source_url
            audio_info = None
            uploadUrl = None
            
            
            
            if( event.source_type == LessonSourceType.youtube):
                audio_info =  media_service.download_youtube_audio(
                    dto.MediaAudioCreateRequest(
                        input_url=event.source_url
                    )
                )
            elif( event.source_type == LessonSourceType.audio_file):
                audio_info =  media_service.download_audio_file(
                    dto.MediaAudioCreateRequest(
                        input_url=event.source_url
                    )
                )
            else:
                raise Exception(f"Unsupported LessonSourceType: {event.source_type}")
            if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
                print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
                return
            uploadUrl = cloud_service.upload_file(
                audio_info.file_path,
                public_id= f"lps/lessons/audio/{audio_info.sourceReferenceId}",
                resource_type= "video" 
            )
            audio_info.audioUrl = uploadUrl
            metadata.source_fetched = audio_info.model_dump(by_alias=True)
            metadataUploadUrl = cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True)),
                public_id= f"lps/lessons/{event.lesson_id}/ai-metadata",
            )
            print(f"‚úÖ ƒê√£ t·∫£i audio cho Lesson voi {event.ai_job_id}, file t·∫°i: {audio_info.file_path}")
        else:
            audio_info = dto.AudioInfo.model_validate(metadata.source_fetched)
            print(f"‚úÖ S·ª≠ d·ª•ng l·∫°i audio_info t·ª´ AI meta data cho Lesson voi {event.ai_job_id}: {audio_info}")
            uploadUrl = audio_info.audioUrl
            isSkip = True
            metadataUploadUrl = event.ai_meta_data_url
        
        
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        
            
            
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.SOURCE_FETCHED,
                audioUrl=uploadUrl,
                sourceReferenceId=audio_info.sourceReferenceId,
                aiMessage="Audio source fetched successfully.",
                thumbnailUrl=audio_info.thumbnailUrl,
                isSkip=isSkip,
                aiMetadataUrl=metadataUploadUrl,
            )
        )
        isSkip = False
        print(f"‚úÖ ƒê√£ g·ª≠i LessonProcessingStepUpdatedEvent SOURCE_FETCHED cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
        # STEP 2: X·ª≠ ly audio b·∫±ng AI
        # STEP 3: NLP analysis
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω LessonGenerationRequestedEvent: {e}")
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiMessage=f"Lesson generation failed: {e}",
                processingStep=LessonProcessingStep.FAILED,
                aiJobId=event.ai_job_id,
            )
        )

async def consume_events():
    """
    M·ªôt consumer duy nh·∫•t l·∫Øng nghe T·∫§T C·∫¢ c√°c topic nghi·ªáp v·ª•.
    """
    topics = [LESSON_GENERATION_REQUESTED_TOPIC]
    
    # Ch·∫°y h√†m blocking `create_kafka_consumer` trong thread ri√™ng
    consumer = await asyncio.to_thread(create_kafka_consumer, topics)
    print(f"üöÄ Kafka consumer (g·ªôp) ƒë√£ kh·ªüi ƒë·ªông, l·∫Øng nghe: {topics}")

    try:
        while True:
            # Ch·∫°y h√†m blocking `poll` trong thread ri√™ng
            # Event loop ch√≠nh ho√†n to√†n r·∫£nh ƒë·ªÉ x·ª≠ l√Ω API (0.27ms)
            msg = await asyncio.to_thread(consumer.poll, 0.1) # 100ms timeout
            
            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                print(f"Kafka error: {msg.error()}")
                continue
            
            # X√°c ƒë·ªãnh xem event ƒë·∫øn t·ª´ topic n√†o
            topic = msg.topic()

            try:
                payload = json.loads(msg.value().decode("utf-8"))

                # Ph√¢n lu·ªìng nghi·ªáp v·ª• d·ª±a tr√™n topic
                if topic == LESSON_GENERATION_REQUESTED_TOPIC:
                    event = LessonGenerationRequestedEvent(**payload)
                    asyncio.create_task(
                        handleLessonGenerationRequested(event)
                    )

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω message (topic: {topic}): {e}")

    except asyncio.CancelledError:
        print("üì™ ƒêang d·ª´ng consumer...")
    finally:
        # Ch·∫°y h√†m blocking `close` trong thread ri√™ng
        await asyncio.to_thread(consumer.close)
        print("üì™ Consumer ƒë√£ d·ª´ng.")


async def start_kafka_consumers():
    """
    H√†m n√†y ƒë∆∞·ª£c g·ªçi b·ªüi `lifespan` trong `main.py`
    """
    # Ch·ªâ c·∫ßn ch·∫°y 1 consumer g·ªôp duy nh·∫•t
    await consume_events()

