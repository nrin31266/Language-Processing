# src/kafka/consumer.py

import asyncio
import json
from typing import List

from confluent_kafka import KafkaError

from src import dto
from src.enum import LessonProcessingStep, LessonSourceType
from src.kafka.config import create_kafka_consumer
from src.kafka.event import (
    LessonGenerationRequestedEvent,
    LessonProcessingStepUpdatedEvent,
)
from src.kafka.producer import publish_lesson_processing_step_updated
from src.kafka.topic import (
    LESSON_GENERATION_REQUESTED_TOPIC,
)
from src.services import (
    media_service,
    ai_job_service,
    speech_to_text_service,
    batch_service,
)
from src.services.file_service import fetch_json_from_url, file_exists
from src.s3_storage import cloud_service
from src.utils.chunk_utils import chunk_list


async def handleLessonGenerationRequested(event: LessonGenerationRequestedEvent):
    """Xá»­ lÃ½ khi cÃ³ yÃªu cáº§u táº¡o bÃ i há»c."""
    print(f"ğŸ“¥ Nháº­n LessonGenerationRequestedEvent: {event}")
    audio_info = None  # cho finally náº¿u sau nÃ y dá»n file

    try:
        # Cho 2s cho há»‡ thá»‘ng á»•n Ä‘á»‹nh
        await asyncio.sleep(2)

        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
            return

        isSkip = False
        metadata: dto.AiMetadataDto

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 0. Láº¥y metadata ban Ä‘áº§u (náº¿u cÃ³)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            fileMetadata = await fetch_json_from_url(event.ai_meta_data_url)
            if fileMetadata:
                metadata = dto.AiMetadataDto.model_validate(fileMetadata)
                print(f"âœ… Fetched AI meta data from URL {event.ai_meta_data_url}")
            else:
                metadata = dto.AiMetadataDto()
        except Exception:
            metadata = dto.AiMetadataDto()  # Táº¡o object rá»—ng Ä‘á»ƒ trÃ¡nh None

        uploadUrl = None
        metadataUploadUrl = event.ai_meta_data_url if event.ai_meta_data_url else None

        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
            return

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 1: Download audio tá»« source_url
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if metadata.sourceFetched is None or event.is_restart:
            print(f"ğŸ” Báº¯t Ä‘áº§u táº£i audio tá»« source_url cho Lesson id {event.lesson_id}")

            if event.source_type == LessonSourceType.youtube:
                audio_info = await media_service.download_youtube_audio(
                    dto.MediaAudioCreateRequest(input_url=event.source_url)
                )
            elif event.source_type == LessonSourceType.audio_file:
                audio_info = await media_service.download_audio_file(
                    dto.MediaAudioCreateRequest(input_url=event.source_url)
                )
            else:
                raise Exception(f"Unsupported LessonSourceType: {event.source_type}")

            if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
                print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
                return

            # Upload audio lÃªn Cloudinary (async)
            uploadUrl = await cloud_service.upload_file(
                audio_info.file_path,
                public_id=f"lps/lessons/audio/{audio_info.sourceReferenceId}",
                resource_type="video",
            )

            audio_info.audioUrl = uploadUrl
            metadata.sourceFetched = dto.SourceFetchedDto.model_validate(
                audio_info.model_dump(by_alias=True)
            )

            # Upload metadata lÃªn Cloudinary (async)
            metadataUploadUrl = await cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True), ensure_ascii=False),
                public_id=f"lps/lessons/{event.lesson_id}/ai-metadata",
            )
            print(
                f"âœ… ÄÃ£ táº£i audio cho Lesson vá»›i ai_job_id {event.ai_job_id}, "
                f"file táº¡i: {audio_info.file_path}"
            )
        else:
            # DÃ¹ng láº¡i metadata Ä‘Ã£ cÃ³
            audio_info = dto.AudioInfo.model_validate(metadata.sourceFetched)
            print(
                f"ğŸ” Sá»­ dá»¥ng láº¡i audio_info tá»« AI meta data cho Lesson vá»›i "
                f"ai_job_id {event.ai_job_id}: {audio_info}"
            )
            uploadUrl = audio_info.audioUrl
            isSkip = True
            metadataUploadUrl = event.ai_meta_data_url

            # Kiá»ƒm tra file audio local cÃ³ tá»“n táº¡i khÃ´ng, náº¿u khÃ´ng thÃ¬ táº£i láº¡i
            if not file_exists(audio_info.file_path):
                print(
                    f"âš ï¸ File audio local khÃ´ng tá»“n táº¡i táº¡i {audio_info.file_path}, "
                    f"sáº½ táº£i láº¡i tá»« audioUrl."
                )
                downloaded = await media_service.download_audio_file(
                    dto.MediaAudioCreateRequest(
                        input_url=audio_info.audioUrl,
                        audio_name=audio_info.sourceReferenceId,
                    )
                )
                audio_info.file_path = downloaded.file_path

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1.1. Bá»• sung duration náº¿u thiáº¿u
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if metadata.sourceFetched.duration is None:
            duration = await speech_to_text_service.get_audio_duration(audio_info.file_path)
            metadata.sourceFetched.duration = int(duration)
            print(
                f"ğŸ” Láº¥y duration cho audio táº¡i {audio_info.file_path}. "
                f"Duration: {metadata.sourceFetched.duration}"
            )

        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
            return

        # Notify SOURCE_FETCHED
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.SOURCE_FETCHED,
                audioUrl=uploadUrl,
                sourceReferenceId=audio_info.sourceReferenceId,
                aiMessage="Audio source fetched successfully.",
                thumbnailUrl=audio_info.thumbnailUrl,
                isSkip=isSkip,
                aiMetadataUrl=metadataUploadUrl,
                durationSeconds=metadata.sourceFetched.duration
                if metadata.sourceFetched.duration
                else 0,
            )
        )
        isSkip = False
        print(
            f"âœ… ÄÃ£ gá»­i LessonProcessingStepUpdatedEvent SOURCE_FETCHED "
            f"cho Lesson vá»›i ai_job_id: {event.ai_job_id}"
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 2: Xá»­ lÃ½ audio báº±ng AI (transcribe)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        transcription_result: dto.TranscribedDto

        if metadata.transcribed is None or event.is_restart:
            print(f"ğŸ” Báº¯t Ä‘áº§u transcribe audio cho Lesson vá»›i ai_job_id: {event.ai_job_id}")

            raw_transcription = await speech_to_text_service.transcribe(audio_info.file_path)

            # raw_transcription cÃ³ thá»ƒ lÃ  dict, convert sang DTO
            metadata.transcribed = dto.TranscribedDto.model_validate(raw_transcription)
            transcription_result = metadata.transcribed

            print(f"âœ… Audio transcribed for Lesson with ai_job_id: {event.ai_job_id}")

            # Cáº­p nháº­t láº¡i metadata lÃªn cloud, tráº£ vá» cÃ¹ng Ä‘Æ°á»ng dáº«n cÅ©
            metadataUploadUrl = await cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True), ensure_ascii=False),
                public_id=f"lps/lessons/{event.lesson_id}/ai-metadata",
            )
            print(
                f"âœ… Cáº­p nháº­t AI meta data lÃªn {metadataUploadUrl} "
                f"cho Lesson vá»›i ai_job_id: {event.ai_job_id}"
            )
        else:
            print(
                f"ğŸ” Sá»­ dá»¥ng láº¡i transcription tá»« AI meta data cho "
                f"Lesson vá»›i ai_job_id: {event.ai_job_id}"
            )
            isSkip = True
            transcription_result = metadata.transcribed

        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
            return

        # Notify TRANSCRIBED
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.TRANSCRIBED,
                aiMessage="Audio transcribed successfully.",
                audioUrl=uploadUrl,
                isSkip=isSkip,
                aiMetadataUrl=metadataUploadUrl,
            )
        )
        isSkip = False
        print(
            f"âœ… ÄÃ£ gá»­i LessonProcessingStepUpdatedEvent TRANSCRIBED "
            f"cho Lesson vá»›i ai_job_id: {event.ai_job_id}"
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # STEP 3: NLP analysis (cháº¡y batch song song)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        nlp_result: dto.NlpAnalyzedDto

        segments = transcription_result.segments
        sentences_payload = [
            {"orderIndex": idx, "text": seg.text} for idx, seg in enumerate(segments)
        ]

        batch_size = 10
        max_concurrency = 3  # muá»‘n 3 batch song song

        if metadata.nlpAnalyzed is None or event.is_restart:
            print(
                f"ğŸ” Báº¯t Ä‘áº§u NLP analysis cho Lesson vá»›i ai_job_id: {event.ai_job_id}"
            )

            chunks = list(chunk_list(sentences_payload, batch_size))
            nlp_sentences: List[dto.SentenceAnalyzedDto] = []

            # cháº¡y tá»«ng "wave", má»—i wave tá»‘i Ä‘a 3 chunk
            for i in range(0, len(chunks), max_concurrency):
                if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
                    print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng NLP.")
                    return

                wave = chunks[i : i + max_concurrency]

                print(
                    f"ğŸ§  NLP wave {i // max_concurrency + 1}: "
                    f"{wave[0][0]['orderIndex']} â†’ {wave[-1][-1]['orderIndex']} running..."
                )

                # táº¡o tasks cho tá»«ng chunk trong wave
                tasks = [
                    batch_service.analyze_sentence_batch(chunk)
                    for chunk in wave
                ]

                wave_results = await asyncio.gather(*tasks, return_exceptions=True)

                for chunk, result in zip(wave, wave_results):
                    if isinstance(result, Exception):
                        # tuá»³ báº¡n xá»­ lÃ½: raise luÃ´n hay log + skip
                        print(f"âš ï¸ Lá»—i NLP batch {chunk[0]['orderIndex']} â†’ {chunk[-1]['orderIndex']}: {result}")
                        raise result

                    for item in result:
                        nlp_sentences.append(dto.SentenceAnalyzedDto(**item))

            # sáº¯p xáº¿p láº¡i cho cháº¯c (náº¿u sau nÃ y cáº§n guarantee order)
            nlp_sentences.sort(key=lambda s: s.orderIndex)

            nlp_result = dto.NlpAnalyzedDto(sentences=nlp_sentences)

            # LÆ°u vÃ o metadata
            metadata.nlpAnalyzed = dto.NlpAnalyzedDto.model_validate(
                nlp_result.model_dump()
            )

            metadataUploadUrl = await cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True), ensure_ascii=False),
                public_id=f"lps/lessons/{event.lesson_id}/ai-metadata",
            )

            print(
                f"âœ… NLP analysis hoÃ n thÃ nh vÃ  Ä‘Ã£ upload metadata lÃªn {metadataUploadUrl}"
            )

            await publish_lesson_processing_step_updated(
                LessonProcessingStepUpdatedEvent(
                    aiJobId=event.ai_job_id,
                    processingStep=LessonProcessingStep.NLP_ANALYZED,
                    aiMessage="NLP analysis completed successfully.",
                    aiMetadataUrl=metadataUploadUrl,
                    isSkip=False,
                )
            )
        else:
            print(
                f"ğŸ” Sá»­ dá»¥ng láº¡i NLP metadata cho ai_job_id: {event.ai_job_id}"
            )
            nlp_result = dto.NlpAnalyzedDto.model_validate(metadata.nlpAnalyzed)

            await publish_lesson_processing_step_updated(
                LessonProcessingStepUpdatedEvent(
                    aiJobId=event.ai_job_id,
                    processingStep=LessonProcessingStep.NLP_ANALYZED,
                    aiMessage="NLP reused from previous metadata.",
                    aiMetadataUrl=event.ai_meta_data_url,
                    isSkip=True,
                )
            )

        print(
            f"âœ… ÄÃ£ gá»­i LessonProcessingStepUpdatedEvent NLP_ANALYZED "
            f"cho Lesson vá»›i ai_job_id: {event.ai_job_id}"
        )

        # Cho 2s cho há»‡ thá»‘ng á»•n Ä‘á»‹nh, sau Ä‘Ã³ gá»­i COMPLETED
        await asyncio.sleep(2)
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"âš ï¸ AI Job {event.ai_job_id} Ä‘Ã£ bá»‹ há»§y, dá»«ng xá»­ lÃ½.")
            return

        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.COMPLETED,
                aiMessage="Lesson generation completed successfully.",
                aiMetadataUrl=metadataUploadUrl,
                isSkip=False,
            )
        )

    except Exception as e:
        print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ LessonGenerationRequestedEvent: {e}")
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiMessage=f"Lesson generation failed: {e}",
                processingStep=LessonProcessingStep.FAILED,
                aiJobId=event.ai_job_id,
            )
        )
    finally:
        # Náº¿u sau nÃ y muá»‘n dá»n file local thÃ¬ má»Ÿ láº¡i Ä‘oáº¡n nÃ y
        # if audio_info and audio_info.file_path:
        #     try:
        #         file_service.remove_local_file(audio_info.file_path)
        #     except Exception as e:
        #         print(f"âš ï¸ Lá»—i khi xÃ³a file local: {e}")
        print(
            f"ğŸ§¹ HoÃ n táº¥t xá»­ lÃ½ LessonGenerationRequestedEvent cho ai_job_id: {event.ai_job_id}"
        )


async def consume_events():
    """
    Má»™t consumer duy nháº¥t láº¯ng nghe Táº¤T Cáº¢ cÃ¡c topic nghiá»‡p vá»¥.
    """
    topics = [LESSON_GENERATION_REQUESTED_TOPIC]

    # Cháº¡y hÃ m blocking `create_kafka_consumer` trong thread riÃªng
    consumer = await asyncio.to_thread(create_kafka_consumer, topics)
    print(f"ğŸš€ Kafka consumer (gá»™p) Ä‘Ã£ khá»Ÿi Ä‘á»™ng, láº¯ng nghe: {topics}")

    try:
        while True:
            # Cháº¡y hÃ m blocking `poll` trong thread riÃªng
            msg = await asyncio.to_thread(consumer.poll, 0.1)  # 100ms timeout

            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                print(f"Kafka error: {msg.error()}")
                continue

            topic = msg.topic()

            try:
                payload = json.loads(msg.value().decode("utf-8"))

                if topic == LESSON_GENERATION_REQUESTED_TOPIC:
                    event = LessonGenerationRequestedEvent(**payload)
                    asyncio.create_task(handleLessonGenerationRequested(event))

            except Exception as e:
                print(f"âš ï¸ Lá»—i xá»­ lÃ½ message (topic: {topic}): {e}")

    except asyncio.CancelledError:
        print("ğŸ“ª Äang dá»«ng consumer...")
    finally:
        await asyncio.to_thread(consumer.close)
        print("ğŸ“ª Consumer Ä‘Ã£ dá»«ng.")


async def start_kafka_consumers():
    """
    HÃ m nÃ y Ä‘Æ°á»£c gá»i bá»Ÿi `lifespan` trong `main.py`
    """
    await consume_events()
