# src/kafka/consumer.py

import asyncio
import json
from src.kafka.config import create_kafka_consumer
from src.kafka.event import (
    LessonGenerationRequestedEvent,
    LessonProcessingStepUpdatedEvent
)
from src.kafka.producer import (
    publish_lesson_processing_step_updated
)
from src.enum import LessonProcessingStep, LessonSourceType
from confluent_kafka import KafkaError
from src.kafka.topic import LESSON_GENERATION_REQUESTED_TOPIC, LESSON_PROCESSING_STEP_UPDATED_TOPIC
import uuid
from typing import List
from src import dto
from src.services import media_service
from src.s3_storage import cloud_service
from src.services import ai_job_service
from src.services import file_service
from src.services.lesson_service import lessonParseAiMetaData
from src.services.file_service import fetch_json_from_url, file_exists
from src.services import speech_to_text_service
from src.services import batch_service
from src.utils.chunk_utils import chunk_list

async def handleLessonGenerationRequested(event: LessonGenerationRequestedEvent):
    """X·ª≠ l√Ω khi c√≥ y√™u c·∫ßu t·∫°o b√†i h·ªçc."""
    print(f"üì• Nh·∫≠n LessonGenerationRequestedEvent: {event}")
    try:
        # Cho 2s cho h·ªá th·ªëng ·ªïn ƒë·ªãnh
        await asyncio.sleep(2)
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        isSkip = False
        metadata : dto.AiMetadataDto = None
        try:
            fileMetadata = await fetch_json_from_url(event.ai_meta_data_url)
            metadata = dto.AiMetadataDto.model_validate(fileMetadata)
            print(f"‚úÖ Fetched AI meta data from URL {event.ai_meta_data_url}")
            # print(f"üîç AI Meta Data: {metadata.model_dump()}")
        except Exception as e:
            metadata = dto.AiMetadataDto()   # T·∫°o object r·ªóng ƒë·ªÉ tr√°nh None

        audio_info = None
        uploadUrl = None
        metadataUploadUrl = event.ai_meta_data_url if event.ai_meta_data_url else None
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        
        if metadata.sourceFetched is None or event.is_restart:
            # STEP 1: Download audio t·ª´ source_url
            
            if( event.source_type == LessonSourceType.youtube):
                audio_info =  media_service.download_youtube_audio(
                    dto.MediaAudioCreateRequest(
                        input_url=event.source_url
                    )
                )
            elif( event.source_type == LessonSourceType.audio_file):
                audio_info =  media_service.download_audio_file(
                    dto.MediaAudioCreateRequest(
                        input_url=event.source_url
                    )
                )
            else:
                raise Exception(f"Unsupported LessonSourceType: {event.source_type}")
            
            if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
                print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
                return
            
            uploadUrl = cloud_service.upload_file(
                audio_info.file_path,
                public_id= f"lps/lessons/audio/{audio_info.sourceReferenceId}",
                resource_type= "video" 
            )
            
            audio_info.audioUrl = uploadUrl
            metadata.sourceFetched = dto.SourceFetchedDto.model_validate(
                audio_info.model_dump(by_alias=True)
            )
        
            metadataUploadUrl = cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True)),
                public_id= f"lps/lessons/{event.lesson_id}/ai-metadata",
            )
            print(f"‚úÖ ƒê√£ t·∫£i audio cho Lesson voi {event.ai_job_id}, file t·∫°i: {audio_info.file_path}")
        else:
            audio_info = dto.AudioInfo.model_validate(metadata.sourceFetched)
            print(f"üîÅ S·ª≠ d·ª•ng l·∫°i audio_info t·ª´ AI meta data cho Lesson voi {event.ai_job_id}: {audio_info}")
            uploadUrl = audio_info.audioUrl
            isSkip = True
            metadataUploadUrl = event.ai_meta_data_url
            # Ki·ªÉm tra file audio c√≥ t·ªìn t·∫°i kh√¥ng
            if not file_exists(audio_info.file_path):
                print(f"‚ö†Ô∏è File audio local kh√¥ng t·ªìn t·∫°i t·∫°i {audio_info.file_path}, s·∫Ω t·∫£i l·∫°i t·ª´ source_url. Download l·∫°i.")
                audio_info.file_path =  media_service.download_audio_file(
                    dto.MediaAudioCreateRequest(
                        input_url=audio_info.audioUrl,
                        audio_name=audio_info.sourceReferenceId
                    )
                ).file_path
    
        if( metadata.sourceFetched.duration is None):
            metadata.sourceFetched.duration = int(speech_to_text_service.get_audio_duration(audio_info.file_path))
            print(f"üîç L·∫•y duration cho audio t·∫°i {audio_info.file_path}. Duration: {metadata.sourceFetched.duration}")

        
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        
        
            
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.SOURCE_FETCHED,
                audioUrl=uploadUrl,
                sourceReferenceId=audio_info.sourceReferenceId,
                aiMessage="Audio source fetched successfully.",
                thumbnailUrl=audio_info.thumbnailUrl,
                isSkip=isSkip,
                aiMetadataUrl=metadataUploadUrl,
                # always int 
                durationSeconds=metadata.sourceFetched.duration if metadata.sourceFetched.duration else 0
            )
        )
        isSkip = False
        print(f"‚úÖ ƒê√£ g·ª≠i LessonProcessingStepUpdatedEvent sourceFetched cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
        # STEP 2: X·ª≠ ly audio b·∫±ng AI
        transcription_result: dto.TranscribedDto = None
        if( metadata.transcribed is None or event.is_restart):
            print(f"üîç B·∫Øt ƒë·∫ßu transcribe audio cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
            transcription_result = speech_to_text_service.transcribe(
                audio_info.file_path,
            )
        
            metadata.transcribed = dto.TranscribedDto.model_validate(transcription_result)
            transcription_result = metadata.transcribed
            print(f"‚úÖ Audio transcribed for Lesson with ai_job_id: {event.ai_job_id}")
            # C·∫≠p nh·∫≠t l·∫°i metadata l√™n cloud tra ve cung duong dan cu
            metadataUploadUrl = cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True), ensure_ascii=False),
                public_id= f"lps/lessons/{event.lesson_id}/ai-metadata",
            )
            print(f"‚úÖ C·∫≠p nh·∫≠t AI meta data l√™n {metadataUploadUrl} cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
        else:
            print(f"üîÅ S·ª≠ d·ª•ng l·∫°i transcription t·ª´ AI meta data cho Lesson voi {event.ai_job_id}")
            isSkip = True
            transcription_result = metadata.transcribed
        
        if( await ai_job_service.aiJobWasCancelled(event.ai_job_id)):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.TRANSCRIBED,
                aiMessage="Audio transcribed successfully.",
                audioUrl=uploadUrl,
                isSkip=isSkip,
                aiMetadataUrl=metadataUploadUrl,
            )
        )
        isSkip = False
        print(f"‚úÖ ƒê√£ g·ª≠i LessonProcessingStepUpdatedEvent TRANSCRIBED cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
        
        # STEP 3: NLP analysis
        nlp_result: dto.NlpAnalyzedDto = None

        segments = transcription_result.segments
        nlp_sentences: List[dto.SentenceAnalyzedDto] = []
        batch_size = 5

        # Build payload v·ªõi index chu·∫©n
        sentences_payload = [
            {"orderIndex": idx, "text": seg.text}
            for idx, seg in enumerate(segments)
        ]


        if metadata.nlpAnalyzed is None or event.is_restart:
            print(f"üîç B·∫Øt ƒë·∫ßu NLP analysis cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")

            for chunk in chunk_list(sentences_payload, batch_size):

                # 1) Check cancel tr∆∞·ªõc m·ªói batch
                if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
                    print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng NLP.")
                    return

                print(f"üß† NLP batch {chunk[0]['orderIndex']} ‚Üí {chunk[-1]['orderIndex']} running...")
                # 2) G·ª≠i batch sang Gemini
                batch_output = await batch_service.analyze_sentence_batch(chunk)

                # 3) Convert sang DTO
                for item in batch_output:
                    nlp_sentences.append(dto.SentenceAnalyzedDto(**item))

                await asyncio.sleep(0.1)   # gi·∫£m spam API

            # Full NLP result
            nlp_result = dto.NlpAnalyzedDto(sentences=nlp_sentences)

            # L∆∞u v√†o metadata
            metadata.nlpAnalyzed = dto.NlpAnalyzedDto.model_validate(nlp_result.model_dump())

            # Upload metadata m·ªõi
            metadataUploadUrl = cloud_service.upload_json_content(
                json.dumps(metadata.model_dump(by_alias=True), ensure_ascii=False),
                public_id=f"lps/lessons/{event.lesson_id}/ai-metadata",
            )

            print(f"‚úÖ NLP analysis ho√†n th√†nh v√† ƒë√£ upload metadata l√™n {metadataUploadUrl}")

            await publish_lesson_processing_step_updated(
                LessonProcessingStepUpdatedEvent(
                    aiJobId=event.ai_job_id,
                    processingStep=LessonProcessingStep.NLP_ANALYZED,
                    aiMessage="NLP analysis completed successfully.",
                    aiMetadataUrl=metadataUploadUrl,
                    isSkip=False
                )
            )

        else:
            print(f"üîÅ S·ª≠ d·ª•ng l·∫°i NLP metadata cho ai_job_id: {event.ai_job_id}")
            nlp_result = dto.NlpAnalyzedDto.model_validate(metadata.nlpAnalyzed)

            await publish_lesson_processing_step_updated(
                LessonProcessingStepUpdatedEvent(
                    aiJobId=event.ai_job_id,
                    processingStep=LessonProcessingStep.NLP_ANALYZED,
                    aiMessage="NLP reused from previous metadata.",
                    aiMetadataUrl=event.ai_meta_data_url,
                    isSkip=True
                )
            )
        print(f"‚úÖ ƒê√£ g·ª≠i LessonProcessingStepUpdatedEvent nlpAnalyzed cho Lesson v·ªõi ai_job_id: {event.ai_job_id}")
        # Cho 2s cho h·ªá th·ªëng ·ªïn ƒë·ªãnh, sau do gui complete
        await asyncio.sleep(2)
        if await ai_job_service.aiJobWasCancelled(event.ai_job_id):
            print(f"‚ö†Ô∏è AI Job {event.ai_job_id} ƒë√£ b·ªã h·ªßy, d·ª´ng x·ª≠ l√Ω.")
            return
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiJobId=event.ai_job_id,
                processingStep=LessonProcessingStep.COMPLETED,
                aiMessage="Lesson generation completed successfully.",
                aiMetadataUrl=metadataUploadUrl,
                isSkip=False
            )
        )
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω LessonGenerationRequestedEvent: {e}")
        await publish_lesson_processing_step_updated(
            LessonProcessingStepUpdatedEvent(
                aiMessage=f"Lesson generation failed: {e}",
                processingStep=LessonProcessingStep.FAILED,
                aiJobId=event.ai_job_id,
            )
        )
    finally:
        # D·ªçn d·∫πp file local, try except ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng l·ªói
        # if audio_info and audio_info.file_path:
        #     try:
        #         file_service.remove_local_file(audio_info.file_path)
        #     except Exception as e:
        #         print(f"‚ö†Ô∏è L·ªói khi x√≥a file local: {e}")
        print(f"üßπ Ho√†n t·∫•t x·ª≠ l√Ω LessonGenerationRequestedEvent cho ai_job_id: {event.ai_job_id}")

async def consume_events():
    """
    M·ªôt consumer duy nh·∫•t l·∫Øng nghe T·∫§T C·∫¢ c√°c topic nghi·ªáp v·ª•.
    """
    topics = [LESSON_GENERATION_REQUESTED_TOPIC]
    
    # Ch·∫°y h√†m blocking `create_kafka_consumer` trong thread ri√™ng
    consumer = await asyncio.to_thread(create_kafka_consumer, topics)
    print(f"üöÄ Kafka consumer (g·ªôp) ƒë√£ kh·ªüi ƒë·ªông, l·∫Øng nghe: {topics}")

    try:
        while True:
            # Ch·∫°y h√†m blocking `poll` trong thread ri√™ng
            # Event loop ch√≠nh ho√†n to√†n r·∫£nh ƒë·ªÉ x·ª≠ l√Ω API (0.27ms)
            msg = await asyncio.to_thread(consumer.poll, 0.1) # 100ms timeout
            
            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                print(f"Kafka error: {msg.error()}")
                continue
            
            # X√°c ƒë·ªãnh xem event ƒë·∫øn t·ª´ topic n√†o
            topic = msg.topic()

            try:
                payload = json.loads(msg.value().decode("utf-8"))

                # Ph√¢n lu·ªìng nghi·ªáp v·ª• d·ª±a tr√™n topic
                if topic == LESSON_GENERATION_REQUESTED_TOPIC:
                    event = LessonGenerationRequestedEvent(**payload)
                    asyncio.create_task(
                        handleLessonGenerationRequested(event)
                    )

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω message (topic: {topic}): {e}")

    except asyncio.CancelledError:
        print("üì™ ƒêang d·ª´ng consumer...")
    finally:
        # Ch·∫°y h√†m blocking `close` trong thread ri√™ng
        await asyncio.to_thread(consumer.close)
        print("üì™ Consumer ƒë√£ d·ª´ng.")


async def start_kafka_consumers():
    """
    H√†m n√†y ƒë∆∞·ª£c g·ªçi b·ªüi `lifespan` trong `main.py`
    """
    # Ch·ªâ c·∫ßn ch·∫°y 1 consumer g·ªôp duy nh·∫•t
    await consume_events()

